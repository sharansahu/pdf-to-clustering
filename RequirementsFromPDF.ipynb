{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpecPDF Requirements Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression library, used for pattern matching and text manipulation.\n",
    "%pip install regex\n",
    "\n",
    "# PyMuPDF library, used for working with PDF files.\n",
    "%pip install PyMuPDF\n",
    "\n",
    "# Pandas library, used for data manipulation and analysis with powerful data structures like DataFrames.\n",
    "%pip install pandas\n",
    "\n",
    "# SentenceTransformer library, used for encoding sentences into numerical vectors for NLP tasks.\n",
    "%pip install sentence-transformers\n",
    "\n",
    "# HDBSCAN, library, used for hierarchical clustering on high dimensional data without encountering curse of dimensionality\n",
    "%pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "\n",
    "# Regular expression library, used for pattern matching and text manipulation.\n",
    "import re\n",
    "\n",
    "# PyMuPDF library, used for working with PDF files.\n",
    "import fitz\n",
    "\n",
    "# Provides a way to interact with the operating system, enabling file and directory operations, among other things.\n",
    "import os\n",
    "\n",
    "# Pandas library, used for data manipulation and analysis with powerful data structures like DataFrames.\n",
    "import pandas\n",
    "\n",
    "# SentenceTransformer library, used for encoding sentences into numerical vectors for NLP tasks.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Import the DBSCAN clustering algorithm from scikit-learn (a machine learning library)\n",
    "# This will be used as an alternative clustering method for the system requirements.\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Import the HDBSCAN library for Hierarchical Density-Based Spatial Clustering of Applications with Noise\n",
    "# This library provides an implementation of the HDBSCAN clustering algorithm, which can be used for clustering the system requirements based on sentence embeddings.\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract requirements from a PDF document and print them\n",
    "\n",
    "# Path to the PDF document\n",
    "DATA_PATH = os.getcwd() + \"/SpecPage.pdf\"\n",
    "\n",
    "# Open the PDF document\n",
    "doc = fitz.open(DATA_PATH)\n",
    "\n",
    "# Initialize an empty string to store the extracted text\n",
    "text = \"\"\n",
    "\n",
    "# Iterate over each page in the document and extract the text\n",
    "for page in doc:\n",
    "    text += page.get_text()\n",
    "\n",
    "# Use regular expressions to extract the requirements based on a specific pattern\n",
    "requirements = re.findall(r\"\\d+\\.\\d+\\.\\d{2}\\s(.*?)(?=\\s\\d+\\.\\d+\\.\\d{2}\\s|$)\", text, re.DOTALL)\n",
    "\n",
    "# Remove leading/trailing whitespace and line breaks from each requirement\n",
    "requirements = [requirement.strip() for requirement in requirements]\n",
    "\n",
    "# Prepend the number to the first requirement using regex matching\n",
    "requirements[0] = re.search(r\"\\d+\\.\\d+\\.\\d{2}\", text).group() + \" \" + requirements[0]\n",
    "\n",
    "# Print the extracted requirements\n",
    "for requirement in requirements:\n",
    "    print(requirement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCH System Performance Specification Requirements Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract requirements from a PDF document and print them\n",
    "\n",
    "# Path to the PDF document\n",
    "DATA_PATH = os.getcwd() + \"/SystemSpecPage.pdf\"\n",
    "\n",
    "# Open the PDF document\n",
    "doc = fitz.open(DATA_PATH)\n",
    "\n",
    "# Initialize an empty string to store the extracted text\n",
    "text = \"\"\n",
    "\n",
    "# Iterate over each page in the document and extract the text\n",
    "for page in doc:\n",
    "    text += page.get_text()\n",
    "\n",
    "# Use regular expressions to extract the requirements based on a specific pattern\n",
    "pattern = r\"(?<!UNCLASSIFIED//FOR OFFICIAL USE ONLY\\n)(\\d+\\.\\d+\\.\\d+\\.\\d+)\\s(.*?)(?=\\n\\d+\\.\\d+\\.\\d+\\.\\d+\\s|$)(?!\\nUNCLASSIFIED//FOR OFFICIAL USE ONLY)\"\n",
    "matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "# Initialize an empty list to store the extracted requirements\n",
    "requirements = []\n",
    "\n",
    "# Iterate over each match in the matches list\n",
    "for match in matches:\n",
    "    number = match[0]\n",
    "    subpattern = r\"(UNCLASSIFIED//FOR OFFICIAL USE ONLY\\s+\\d+\\s+)\"\n",
    "    \n",
    "    # Remove the \"UNCLASSIFIED//FOR OFFICIAL USE ONLY\" prefix and leading whitespace from the requirement\n",
    "    sentence = re.sub(subpattern, \"\", match[1].strip())\n",
    "    \n",
    "    # Remove any remaining \"UNCLASSIFIED//FOR OFFICIAL USE ONLY\" occurrences\n",
    "    subpattern = r\"UNCLASSIFIED//FOR OFFICIAL USE ONLY\"\n",
    "    sentence = re.sub(subpattern, \"\", sentence)\n",
    "    \n",
    "    # Check if the requirement contains the \"Acronyms\" keyword\n",
    "    if \"Acronyms\" in sentence:\n",
    "        break\n",
    "    \n",
    "    # Append the cleaned requirement sentence to the requirements list\n",
    "    requirements.append(sentence)\n",
    "    \n",
    "    # Print the number and sentence of the extracted requirement\n",
    "    print(f\"Number: {number}\")\n",
    "    print(f\"Sentence: {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging Similar Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model for sentence embeddings\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# How many samples do you want to process \n",
    "N_EXAMPLES = 20\n",
    "\n",
    "# Select a subset of requirements to process\n",
    "ex_reqs = requirements[:N_EXAMPLES]\n",
    "\n",
    "# Encode subset of requirements as numerical representation\n",
    "sentence_embeddings = model.encode(ex_reqs)\n",
    "\n",
    "# Using DBSCAN for clustering\n",
    "# Create a DBSCAN clustering object with specified hyperparameters.\n",
    "# 'eps' sets the maximum distance between two samples to be considered in the same neighborhood.\n",
    "# 'min_samples' specifies the minimum number of samples in a neighborhood for a point to be considered as a core point.\n",
    "# 'metric' determines the distance metric used for clustering.\n",
    "dbscan_cluster = DBSCAN(eps=3, min_samples=5, metric='euclidean')\n",
    "\n",
    "# Perform clustering on the sentence embeddings using DBSCAN.\n",
    "# The 'fit_predict' method finds the clusters and returns an array of cluster labels for each input sentence.\n",
    "dbscan_labels = dbscan_cluster.fit_predict(sentence_embeddings)\n",
    "\n",
    "# Using HDBSCAN for clustering\n",
    "# Create an HDBSCAN clustering object with specified hyperparameters.\n",
    "# 'min_cluster_size' sets the minimum number of points required to form a cluster in HDBSCAN.\n",
    "# 'metric' determines the distance metric used for clustering.\n",
    "hdbscan_cluster = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean')\n",
    "\n",
    "# Perform clustering on the sentence embeddings using HDBSCAN.\n",
    "# The 'fit_predict' method finds the clusters and returns an array of cluster labels for each input sentence.\n",
    "hdbscan_labels = hdbscan_cluster.fit_predict(sentence_embeddings)\n",
    "\n",
    "# Function to print out the requirements that belong to each cluster.\n",
    "# It takes a list of sentences and an array of cluster labels as input.\n",
    "def print_clusters(sentences, cluster_labels):\n",
    "    clusters = {}\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        if label == -1:  # Outlier points in HDBSCAN are labeled as -1\n",
    "            continue\n",
    "        if label not in clusters:\n",
    "            clusters[label] = []\n",
    "        clusters[label].append(sentences[idx])\n",
    "    \n",
    "    # Sort the clusters based on cluster_id before printing.\n",
    "    sorted_clusters = dict(sorted(clusters.items()))\n",
    "    \n",
    "    for cluster_id, sentences in sorted_clusters.items():\n",
    "        print(f\"Cluster {cluster_id}:\")\n",
    "        for sentence in sentences:\n",
    "            print(sentence)\n",
    "        print()\n",
    "\n",
    "# Print the results for the DBSCAN clustering.\n",
    "print(\"Using DBSCAN:\")\n",
    "print_clusters(ex_reqs, dbscan_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results for the HDBSCAN clustering.\n",
    "print(\"Using HDBSCAN:\")\n",
    "\n",
    "# Call the 'print_clusters' function to print out the requirements that belong to each cluster for HDBSCAN.\n",
    "# It takes a list of sentences ('ex_reqs') and an array of cluster labels ('hdbscan_labels') as input.\n",
    "print_clusters(ex_reqs, hdbscan_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
